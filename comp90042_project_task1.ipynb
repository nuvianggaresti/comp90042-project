{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "comp90042_project-task1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4XmLa8kvvQI"
      },
      "source": [
        "Project 1 Task 1 COMP90042"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbOlnuWxdc9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1e4dba7-d663-4c15-9629-3c2235fa85f5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gegi2qweQXxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3e3030-bf9a-42cf-a963-6defb77842c5"
      },
      "source": [
        "!pip install torch torchvision transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhZhu2GufSJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea06e06-eeb4-4311-f875-8cb2af5c89c6"
      },
      "source": [
        "#load pretrained bert base model\n",
        "from transformers import BertModel\n",
        "\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "print(\"Done loading BERT model.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done loading BERT model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IJ9Sz2VY2Bg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d2ab6d-bfb4-4aab-83e7-6ad82a9c1e73"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import brown\n",
        "\n",
        "import re\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "class RumourDataset(Dataset):\n",
        "    def __init__(self, data_file, label_file, maxlen):\n",
        "        # Open dataset\n",
        "        self.ID = 'id_str'\n",
        "        self.TEXT = 'text'\n",
        "        self.PARENT_ID = 'in_reply_to_status_id_str'\n",
        "        self.PARENT_TWEET = 'text_x'\n",
        "        self.REPLY_TWEETS = 'text_y'\n",
        "        self.LABEL = 'label'\n",
        "\n",
        "        # Libraries\n",
        "        self.lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "        self.stopwords = stopwords.words('english')\n",
        "        \n",
        "        #words from brown corpus\n",
        "        raw_brown_words = brown.words()\n",
        "        brown_words = []\n",
        "        for word in raw_brown_words:\n",
        "          brown_words.append(word.lower())\n",
        "        self.brown_words = brown_words\n",
        "\n",
        "        # Create count_dict, total_word, total_unique_word from brown_corpus\n",
        "        count_dict = {}\n",
        "        total_word = 0\n",
        "        total_unique_word = 0\n",
        "        for word in self.brown_words:\n",
        "            word = word.lower()\n",
        "            if word not in count_dict:\n",
        "                total_unique_word += 1\n",
        "                count_dict[word] = 1\n",
        "            else:\n",
        "                count_dict[word] += 1\n",
        "        for key in count_dict:\n",
        "            total_word += count_dict[key]\n",
        "        self.count_dict = count_dict\n",
        "        self.total_word = total_word\n",
        "        self.total_unique_word = total_unique_word\n",
        "\n",
        "        # Open dataset\n",
        "        self.dataset_df = pd.read_json(path_or_buf=data_file, lines=True) # returns a df where row = a collection of tweets, with each column = a single tweet\n",
        "\n",
        "        # Open label set\n",
        "        if label_file is not None:\n",
        "          with open(label_file) as f:\n",
        "              loaded_json = json.load(f)\n",
        "              label_df = pd.DataFrame.from_dict(loaded_json, orient='index', columns=[self.LABEL])\n",
        "          # Combine dataset and label set\n",
        "          # convert labels into 1s and 0s\n",
        "          label_encoder = preprocessing.LabelEncoder()\n",
        "          label = label_df[self.LABEL]\n",
        "          self.label_dataset = label_encoder.fit_transform(label)\n",
        "        else:\n",
        "          self.label_dataset = None\n",
        "\n",
        "        #Initialize the BERT tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.maxlen = maxlen\n",
        "    \n",
        "    def preprocess_text(self, text):\n",
        "      text = text.lower()\n",
        "      temp = text.split()\n",
        "\n",
        "      for index, word in enumerate(temp):\n",
        "        if word.startswith('@'):\n",
        "          text = text.replace(word, '')\n",
        "        if word.startswith('#'):\n",
        "          new_word = word[1:]\n",
        "          text = text.replace(word, new_word)\n",
        "          # tokens = self.max_match_or_rev(new_word, self.count_dict, self.total_word, self.total_unique_word)\n",
        "          # tokens = self.max_match(new_word)\n",
        "          # tokens_str = ''\n",
        "          # for token in tokens:\n",
        "          #   tokens_str += token + ' '\n",
        "          # text = text.replace(word, tokens_str)\n",
        "\n",
        "      text_url = re.sub(r\"http\\S+\", \"\", text)\n",
        "      \n",
        "      return text_url\n",
        "\n",
        "    def lemmatize(self, word):\n",
        "      lemma = self.lemmatizer.lemmatize(word,'v')\n",
        "      if lemma == word:\n",
        "          lemma = self.lemmatizer.lemmatize(word,'n')\n",
        "      return lemma\n",
        "\n",
        "    def max_match(self, hashtag):\n",
        "        tokens = []\n",
        "        longest_index = 0\n",
        "        while longest_index < len(hashtag):\n",
        "            temp = ''\n",
        "            lemma = ''\n",
        "            longest_word = ''\n",
        "            for i in range(longest_index,len(hashtag)):\n",
        "                temp = hashtag[longest_index:i+1]\n",
        "                lemma = self.lemmatize(temp)\n",
        "                if lemma != temp or lemma in self.brown_words:\n",
        "                    if lemma in self.brown_words:\n",
        "                        longest_word = temp\n",
        "            if len(longest_word) == 0:\n",
        "                longest_word = hashtag[longest_index]\n",
        "            longest_index += len(longest_word)\n",
        "            tokens.append(longest_word)\n",
        "        return tokens\n",
        "\n",
        "    def rev_max_match(self, hashtag):\n",
        "        tokens = []\n",
        "        longest_index = len(hashtag) - 1\n",
        "        while longest_index > -1:\n",
        "            temp = ''\n",
        "            lemma = ''\n",
        "            longest_word = ''\n",
        "            i = longest_index\n",
        "            while i > -1:\n",
        "                temp = hashtag[i:longest_index + 1]\n",
        "                lemma = self.lemmatize(temp)\n",
        "                if lemma != temp or lemma in self.brown_words:\n",
        "                    if lemma in self.brown_words:\n",
        "                        longest_word = temp\n",
        "                i -= 1\n",
        "            if len(longest_word) == 0:\n",
        "                longest_word = hashtag[longest_index]\n",
        "            longest_index -= len(longest_word)\n",
        "            tokens.append(longest_word)\n",
        "        tokens.reverse()\n",
        "        return tokens\n",
        "    \n",
        "    def find_prob(self, tokens, count_dict, total_word, total_unique_word):\n",
        "      # called in max_match_or_rev\n",
        "      prob = 0\n",
        "      for token in tokens:\n",
        "          count = 0\n",
        "          if token in count_dict:\n",
        "              count = count_dict[token] + 1\n",
        "          else:\n",
        "              count += 1\n",
        "          prob += math.log((count / (total_word + total_unique_word)))\n",
        "      return prob\n",
        "\n",
        "    def max_match_or_rev(self, hashtag, count_dict, total_word, total_unique_word):\n",
        "      normal = self.max_match(hashtag)\n",
        "      rev = self.rev_max_match(hashtag)\n",
        "\n",
        "      if normal != rev: # if MaxMatch and reversed MaxMatch differs\n",
        "          normal_prob = self.find_prob(normal, count_dict, total_word, total_unique_word)\n",
        "          rev_prob = self.find_prob(rev, count_dict, total_word, total_unique_word)\n",
        "          if rev_prob > normal_prob:\n",
        "            return rev\n",
        "      return normal\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row_twt = list(self.dataset_df.iloc[index])\n",
        "        parent_tweet = ''\n",
        "        reply_tweets = ''\n",
        "\n",
        "        for twt in row_twt:\n",
        "          if twt == None:\n",
        "            break\n",
        "          \n",
        "          if twt[self.PARENT_ID] == None:\n",
        "            parent_tweet = twt[self.TEXT]\n",
        "            parent_tweet = twt[self.TEXT]\n",
        "          else:\n",
        "            reply_tweets += twt[self.TEXT] + ' '\n",
        "            reply_tweets += twt[self.TEXT] + ' '\n",
        "\n",
        "        # #Preprocessing the text to be suitable for BERT\n",
        "        tokens = self.tokenizer(parent_tweet, \n",
        "                                reply_tweets, \n",
        "                                padding='max_length', \n",
        "                                truncation=True,\n",
        "                                max_length=self.maxlen) #Tokenize the sentence\n",
        "\n",
        "        \n",
        "        # tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
        "        tokens_ids_tensor = torch.tensor(tokens['input_ids']) #Converting the list to a pytorch tensor\n",
        "\n",
        "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
        "        attn_mask = torch.tensor(tokens['attention_mask'])\n",
        "\n",
        "        seg_ids = torch.tensor(tokens['token_type_ids'])\n",
        "        \n",
        "        # Check label, if available return\n",
        "        if self.label_dataset is not None:\n",
        "          label_twt = self.label_dataset[index]\n",
        "          return tokens_ids_tensor, attn_mask, seg_ids, label_twt\n",
        "        else:\n",
        "          return tokens_ids_tensor, attn_mask, seg_ids"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDROtvusaH1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "111f3dab-5641-428d-b2eb-5765df2257e4"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "MAXLEN = 512\n",
        "BATCH_SIZE = 16\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "#Creating instances of training and development set\n",
        "#maxlen sets the maximum length a sentence can have\n",
        "#any sentence longer than this length is truncated to the maxlen size\n",
        "ROOT = '/content/drive/My Drive/nlp-data/'\n",
        "\n",
        "TRAIN_SET = ROOT + 'train.data.jsonl'\n",
        "DEV_SET = ROOT + 'dev.data.jsonl'\n",
        "\n",
        "TRAIN_SET_LABEL = ROOT + 'train.label.json'\n",
        "DEV_SET_LABEL = ROOT + 'dev.label.json'\n",
        "train_set = RumourDataset(data_file = TRAIN_SET, label_file = TRAIN_SET_LABEL, maxlen = MAXLEN)\n",
        "dev_set = RumourDataset(data_file = DEV_SET, label_file = DEV_SET_LABEL, maxlen = MAXLEN)\n",
        "\n",
        "#Creating intsances of training and development dataloaders\n",
        "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS)\n",
        "dev_loader = DataLoader(dev_set, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS)\n",
        "\n",
        "print(\"Done preprocessing training and development data.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done preprocessing training and development data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZG03AXSVPl3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class RumourClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RumourClassifier, self).__init__()\n",
        "        #Instantiating BERT model object \n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "        \n",
        "        #Classification layer\n",
        "        #input dimension is 768 because [CLS] embedding has a dimension of 768\n",
        "        #output dimension is 1 because we're working with a binary classification problem\n",
        "        self.cls_layer = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, seq, attn_masks, seg_ids):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
        "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
        "        '''\n",
        "\n",
        "        #Feeding the input to BERT model to obtain contextualized representations\n",
        "        outputs = self.bert_layer(seq, attention_mask = attn_masks, token_type_ids = seg_ids)\n",
        "        cont_reps = outputs.last_hidden_state\n",
        "\n",
        "        #Obtaining the representation of [CLS] head (the first token)\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "\n",
        "        #Feeding cls_rep to the classifier layer\n",
        "        logits = self.cls_layer(cls_rep)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt1hzRGkVZZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06cef86f-9e38-445f-a37c-8be791b8b303"
      },
      "source": [
        "gpu = 0 #gpu ID\n",
        "\n",
        "print(\"Creating the rumour classifier, initialised with pretrained BERT-BASE parameters...\")\n",
        "net = RumourClassifier()\n",
        "net.cuda(gpu) #Enable gpu support for the model\n",
        "print(\"Done creating the rumour classifier.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating the rumour classifier, initialised with pretrained BERT-BASE parameters...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done creating the rumour classifier.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeddIfMlVc_L"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = optim.Adam(net.parameters(), lr = 2e-5)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SufebKb-VhdQ"
      },
      "source": [
        "import time\n",
        "\n",
        "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
        "    best_acc = 0\n",
        "    st = time.time()\n",
        "    for ep in range(max_eps):\n",
        "        \n",
        "        for it, (seq, attn_masks, seg_ids, labels) in enumerate(train_loader):\n",
        "            #Clear gradients\n",
        "            opti.zero_grad()  \n",
        "            #Converting these to cuda tensors\n",
        "            seq, attn_masks, seg_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), seg_ids.cuda(gpu), labels.cuda(gpu)\n",
        "\n",
        "            #Obtaining the logits from the model\n",
        "            logits = net(seq, attn_masks, seg_ids)\n",
        "\n",
        "            #Computing loss\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "\n",
        "            #Backpropagating the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            #Optimization step\n",
        "            opti.step()\n",
        "              \n",
        "            if it % 100 == 0:\n",
        "                \n",
        "                acc = get_accuracy_from_logits(logits, labels)\n",
        "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
        "                st = time.time()\n",
        "\n",
        "        \n",
        "        dev_acc, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
        "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
        "        torch.save(net.state_dict(), ROOT + 'sstcls_{}.dat'.format(ep))\n",
        "        if dev_acc > best_acc:\n",
        "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
        "            best_acc = dev_acc"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZeEig9YVp4W"
      },
      "source": [
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "def evaluate(net, criterion, dataloader, gpu):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, seg_ids, labels in dataloader:\n",
        "            seq, attn_masks, seg_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), seg_ids.cuda(gpu), labels.cuda(gpu)\n",
        "            logits = net(seq, attn_masks, seg_ids)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
        "            count += 1\n",
        "\n",
        "    return mean_acc / count, mean_loss / count"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Jxx7GjyVsHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb16cb2-18c6-4390-a878-d6753ecf6915"
      },
      "source": [
        "num_epoch = 2\n",
        "\n",
        "#fine-tune the model\n",
        "train(net, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 of epoch 0 complete. Loss: 0.7296366095542908; Accuracy: 0.1875; Time taken (s): 1.1983132362365723\n",
            "Iteration 100 of epoch 0 complete. Loss: 0.14117911458015442; Accuracy: 0.9375; Time taken (s): 83.45783996582031\n",
            "Iteration 200 of epoch 0 complete. Loss: 0.4235019087791443; Accuracy: 0.8125; Time taken (s): 83.43763780593872\n",
            "Epoch 0 complete! Development Accuracy: 0.8665540814399719; Development Loss: 0.2907700264776075\n",
            "Best development accuracy improved from 0 to 0.8665540814399719, saving model...\n",
            "Iteration 0 of epoch 1 complete. Loss: 0.24530521035194397; Accuracy: 0.9375; Time taken (s): 88.22898197174072\n",
            "Iteration 100 of epoch 1 complete. Loss: 0.03604999929666519; Accuracy: 1.0; Time taken (s): 83.6326355934143\n",
            "Iteration 200 of epoch 1 complete. Loss: 0.17435328662395477; Accuracy: 0.9375; Time taken (s): 83.40871739387512\n",
            "Epoch 1 complete! Development Accuracy: 0.8614864945411682; Development Loss: 0.3874914037013376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcVeU44LHiiV"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "ROOT = '/content/drive/My Drive/nlp-data/'\n",
        "\n",
        "def predict_rumour(net, test_set, output_path):\n",
        "  RUMOUR = 'rumour'\n",
        "  NON_RUMOUR = 'non-rumour'\n",
        "\n",
        "  net = net.eval()\n",
        "  test_loader = DataLoader(test_set)\n",
        "  \n",
        "  id_lst = []\n",
        "  for index, row in test_set.dataset_df.iterrows():\n",
        "    for twt in row:\n",
        "      if twt[test_set.PARENT_ID] is None:\n",
        "        id_lst.append(twt[test_set.ID])\n",
        "        break\n",
        "  \n",
        "  predictions_lst = []\n",
        "  with torch.no_grad():\n",
        "      for seq, attn_masks, seg_ids in test_loader:\n",
        "          seq, attn_masks, seg_ids = seq.cuda(gpu), attn_masks.cuda(gpu), seg_ids.cuda(gpu)\n",
        "          logits = net(seq, attn_masks, seg_ids)\n",
        "          probs = torch.sigmoid(logits)\n",
        "          soft_prob = (probs > 0.5).long()\n",
        "          if soft_prob.squeeze().item() == 0:\n",
        "            predictions_lst.append(NON_RUMOUR)\n",
        "          else:\n",
        "            predictions_lst.append(RUMOUR)\n",
        "  predictions_dct = {}\n",
        "  for id, pred in zip(id_lst, predictions_lst):\n",
        "    predictions_dct[id] = pred\n",
        "  \n",
        "  with open(output_path, 'w') as out:\n",
        "    json.dump(predictions_dct, out)\n",
        "  print('Finish Prediction.')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRcsMbAxJu6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fea9f22-e477-4d1f-b161-430617df50e1"
      },
      "source": [
        "ROOT = '/content/drive/My Drive/nlp-data/'\n",
        "OUTPUT = ROOT + 'test-output.json'\n",
        "\n",
        "TEST_SET = ROOT + 'test.data.jsonl'\n",
        "test_set = RumourDataset(data_file = TEST_SET, label_file = None, maxlen = MAXLEN)\n",
        "predict_rumour(net, test_set, OUTPUT)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish Prediction.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlKNnx3ov_B8"
      },
      "source": [
        "Project 1 Task 2 COMP90042"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh4h4Ne_HTck"
      },
      "source": [
        "# Predicting the COVID dataset - beware, huge file\n",
        "# COVID_SET = ROOT + 'covid.data.jsonl'\n",
        "# COVID_OUTPUT = ROOT + 'covid-output.json'\n",
        "\n",
        "# covid_set = RumourDataset(data_file = COVID_SET, label_file = None, maxlen = MAXLEN)\n",
        "# predict_rumour(net, covid_set, COVID_OUTPUT)"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}